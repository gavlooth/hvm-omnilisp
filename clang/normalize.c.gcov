        -:    0:Source:../hvm4/clang/eval/normalize.c
        -:    0:Graph:hvm4-cov-main.gcno
        -:    0:Data:hvm4-cov-main.gcda
        -:    0:Runs:22
        -:    1:// Parallel normalization (SNF) traversal using work-stealing.
        -:    2:#include <pthread.h>
        -:    3:#include <sched.h>
        -:    4:#include <stdatomic.h>
        -:    5:#include <stdbool.h>
        -:    6:
        -:    7:#ifndef EVAL_NORMALIZE_WS_CAP_POW2
        -:    8:#define EVAL_NORMALIZE_WS_CAP_POW2 21
        -:    9:#endif
        -:   10:
        -:   11:#define EVAL_NORMALIZE_SEEN_INIT (1u << 20)
        -:   12:
        -:   13:#define EVAL_NORMALIZE_TASK(loc) ((u64)(loc))
        -:   14:#define EVAL_NORMALIZE_TASK_LOC(task) ((u32)(task))
        -:   15:
        -:   16:typedef struct __attribute__((aligned(64))) {
        -:   17:  WsDeque dq;
        -:   18:  Uset  seen;
        -:   19:} EvalNormalizeWorker;
        -:   20:
        -:   21:typedef struct {
        -:   22:  EvalNormalizeWorker   W[MAX_THREADS];
        -:   23:  u32         n;
        -:   24:  _Atomic u64 pending;
        -:   25:} EvalNormalizeCtx;
        -:   26:
        -:   27:typedef struct {
        -:   28:  EvalNormalizeCtx *ctx;
        -:   29:  u32 tid;
        -:   30:} EvalNormalizeArg;
        -:   31:
       63:   32:static inline void eval_normalize_pending_inc(_Atomic u64 *pending) {
       63:   33:  atomic_fetch_add_explicit(pending, 1, memory_order_relaxed);
       63:   34:}
        -:   35:
       85:   36:static inline void eval_normalize_pending_dec(_Atomic u64 *pending) {
       85:   37:  atomic_fetch_sub_explicit(pending, 1, memory_order_release);
       85:   38:}
        -:   39:
        -:   40:static inline void eval_normalize_par_enqueue(EvalNormalizeCtx *ctx, EvalNormalizeWorker *worker, u64 task);
        -:   41:
       85:   42:static inline void eval_normalize_par_go(EvalNormalizeCtx *ctx, EvalNormalizeWorker *worker, u64 task) {
       85:   43:  u32 loc = EVAL_NORMALIZE_TASK_LOC(task);
       85:   44:  if (loc == 0) {
    #####:   45:    return;
        -:   46:  }
       85:   47:  if (!uset_add(&worker->seen, loc)) {
    #####:   48:    return;
        -:   49:  }
       85:   50:  bool parallel = ctx->n > 1;
      130:   51:  for (;;) {
      215:   52:    Term term = wnf_at(loc);
      215:   53:    u8 tag = term_tag(term);
      215:   54:    if (tag == DP0 || tag == DP1 || tag == GOT) {
        6:   55:      u32 dup_loc = term_val(term);
        6:   56:      if (dup_loc != 0 && !term_sub_get(heap_peek(dup_loc))) {
        6:   57:        if (parallel) {
    #####:   58:          eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(dup_loc));
    #####:   59:          return;
        -:   60:        } else {
        6:   61:          if (!uset_add(&worker->seen, dup_loc)) {
    #####:   62:            return;
        -:   63:          }
        6:   64:          loc = dup_loc;
        6:   65:          continue;
        -:   66:        }
        -:   67:      }
        -:   68:    }
        -:   69:
      209:   70:    u32 ari = term_arity(term);
      209:   71:    if (ari == 0) {
       85:   72:      return;
        -:   73:    }
      124:   74:    u32 tloc = term_val(term);
      124:   75:    if (parallel) {
    #####:   76:      if (tag == DRY) {
    #####:   77:        eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(tloc + 1));
    #####:   78:        eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(tloc));
    #####:   79:        return;
        -:   80:      }
    #####:   81:      if (tag == LAM) {
    #####:   82:        eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(tloc));
    #####:   83:        return;
        -:   84:      }
    #####:   85:      for (u32 i = 0; i < ari; i++) {
    #####:   86:        eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(tloc + i));
        -:   87:      }
    #####:   88:      return;
        -:   89:    }
      124:   90:    if (tag == LAM) {
       11:   91:      if (!uset_add(&worker->seen, tloc)) {
    #####:   92:        return;
        -:   93:      }
       11:   94:      loc = tloc;
       11:   95:      continue;
        -:   96:    }
      113:   97:    if (tag == DRY) {
       12:   98:      eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(tloc + 1));
       12:   99:      if (!uset_add(&worker->seen, tloc)) {
    #####:  100:        return;
        -:  101:      }
       12:  102:      loc = tloc;
       12:  103:      continue;
        -:  104:    }
      152:  105:    for (u32 i = ari; i > 1; i--) {
       51:  106:      eval_normalize_par_enqueue(ctx, worker, EVAL_NORMALIZE_TASK(tloc + (i - 1)));
        -:  107:    }
      101:  108:    if (!uset_add(&worker->seen, tloc)) {
    #####:  109:      return;
        -:  110:    }
      101:  111:    loc = tloc;
        -:  112:  }
        -:  113:}
        -:  114:
       63:  115:static inline void eval_normalize_par_enqueue(EvalNormalizeCtx *ctx, EvalNormalizeWorker *worker, u64 task) {
       63:  116:  if (EVAL_NORMALIZE_TASK_LOC(task) == 0) {
    #####:  117:    return;
        -:  118:  }
       63:  119:  if (wsq_push(&worker->dq, task)) {
       63:  120:    eval_normalize_pending_inc(&ctx->pending);
        -:  121:  } else {
    #####:  122:    eval_normalize_par_go(ctx, worker, task);
        -:  123:  }
        -:  124:}
        -:  125:
       22:  126:static void *eval_normalize_par_worker(void *arg) {
       22:  127:  EvalNormalizeArg *A = (EvalNormalizeArg *)arg;
       22:  128:  EvalNormalizeCtx *ctx = A->ctx;
       22:  129:  u32 me = A->tid;
       22:  130:  EvalNormalizeWorker *worker = &ctx->W[me];
        -:  131:
       22:  132:  wnf_set_tid(me);
        -:  133:
       22:  134:  u32 r = 0x9E3779B9u ^ me;
        -:  135:
       22:  136:  u32 idle = 0;
       85:  137:  for (;;) {
        -:  138:    u64 task;
        -:  139:
      107:  140:    if (wsq_pop(&worker->dq, &task)) {
       85:  141:      eval_normalize_par_go(ctx, worker, task);
       85:  142:      eval_normalize_pending_dec(&ctx->pending);
       85:  143:      idle = 0;
       85:  144:      continue;
        -:  145:    }
        -:  146:
       22:  147:    bool stolen = false;
       22:  148:    u32 n = ctx->n;
       22:  149:    u32 start = (me + 1 + (r & 7)) % n;
       22:  150:    r ^= r << 13;
       22:  151:    r ^= r >> 17;
       22:  152:    r ^= r << 5;
        -:  153:
      22*:  154:    for (u32 k = 0; k < n - 1; k++) {
    #####:  155:      u32 vic = (start + k) % n;
    #####:  156:      if (vic == me) {
    #####:  157:        continue;
        -:  158:      }
    #####:  159:      if (wsq_steal(&ctx->W[vic].dq, &task)) {
    #####:  160:        eval_normalize_par_go(ctx, worker, task);
    #####:  161:        eval_normalize_pending_dec(&ctx->pending);
    #####:  162:        stolen = true;
    #####:  163:        idle = 0;
    #####:  164:        break;
        -:  165:      }
        -:  166:    }
        -:  167:
      22*:  168:    if (stolen) {
    #####:  169:      continue;
        -:  170:    }
        -:  171:
       22:  172:    if (atomic_load_explicit(&ctx->pending, memory_order_acquire) == 0) {
       22:  173:      break;
        -:  174:    }
        -:  175:
    #####:  176:    if (idle < 1024) {
    #####:  177:      cpu_relax();
    #####:  178:      idle++;
        -:  179:    } else {
    #####:  180:      sched_yield();
    #####:  181:      idle = 0;
        -:  182:    }
        -:  183:  }
        -:  184:
       22:  185:  wnf_itrs_flush(me);
       22:  186:  return NULL;
        -:  187:}
        -:  188:
       22:  189:static inline Term eval_normalize_par(Term term) {
       22:  190:  wnf_set_tid(0);
        -:  191:
       22:  192:  u32 root_loc = (u32)heap_alloc(1);
       22:  193:  heap_set(root_loc, term);
        -:  194:
        -:  195:  EvalNormalizeCtx ctx;
        -:  196:
       22:  197:  u32 n = thread_get_count();
       22:  198:  if (n == 0) {
    #####:  199:    n = 1;
        -:  200:  }
       22:  201:  if (n > MAX_THREADS) {
    #####:  202:    n = MAX_THREADS;
        -:  203:  }
       22:  204:  ctx.n = n;
       22:  205:  atomic_store_explicit(&ctx.pending, 0, memory_order_relaxed);
       44:  206:  for (u32 i = 0; i < n; i++) {
       22:  207:    if (!wsq_init(&ctx.W[i].dq, EVAL_NORMALIZE_WS_CAP_POW2)) {
    #####:  208:      fprintf(stderr, "eval_normalize: queue allocation failed\n");
    #####:  209:      exit(1);
        -:  210:    }
       22:  211:    uset_init(&ctx.W[i].seen, EVAL_NORMALIZE_SEEN_INIT);
        -:  212:  }
        -:  213:
       22:  214:  EvalNormalizeWorker *worker0 = &ctx.W[0];
       22:  215:  u64 root_task = EVAL_NORMALIZE_TASK(root_loc);
       22:  216:  if (wsq_push(&worker0->dq, root_task)) {
       22:  217:    atomic_store_explicit(&ctx.pending, 1, memory_order_relaxed);
        -:  218:  } else {
    #####:  219:    eval_normalize_par_go(&ctx, worker0, root_task);
        -:  220:  }
        -:  221:
        -:  222:  pthread_t tids[MAX_THREADS];
        -:  223:  EvalNormalizeArg args[MAX_THREADS];
      22*:  224:  for (u32 i = 1; i < n; i++) {
    #####:  225:    args[i].ctx = &ctx;
    #####:  226:    args[i].tid = i;
    #####:  227:    pthread_create(&tids[i], NULL, eval_normalize_par_worker, &args[i]);
        -:  228:  }
        -:  229:
       22:  230:  EvalNormalizeArg arg0 = { .ctx = &ctx, .tid = 0 };
       22:  231:  eval_normalize_par_worker(&arg0);
       22:  232:  wnf_itrs_flush(0);
        -:  233:
      22*:  234:  for (u32 i = 1; i < n; i++) {
    #####:  235:    pthread_join(tids[i], NULL);
        -:  236:  }
        -:  237:
       44:  238:  for (u32 i = 0; i < n; i++) {
       22:  239:    wsq_free(&ctx.W[i].dq);
       22:  240:    uset_free(&ctx.W[i].seen);
        -:  241:  }
        -:  242:
       22:  243:  return heap_read(root_loc);
        -:  244:}
        -:  245:
       22:  246:fn Term eval_normalize(Term term) {
       22:  247:  return eval_normalize_par(term);
        -:  248:}
